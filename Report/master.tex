%\documentclass[11pt]{report}
%\usepackage[a4paper]{geometry}
\input{prelude}

%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT %
%%%%%%%%%%%%%%%%%%

%\usepackage[final]{listings}

%\input{csvfiles}

\begin{document}

\maketitle
%\frontmatter
%\makedecl
\input{acknowledgement}
%\include{abstract}
%\mainmatter
%\setcounter{tocdepth}{3}
%\setcounter{secnumdepth}{3} dunno what this does.
{
  \hypersetup{linkcolor=black}
  \tableofcontents
  % Two times compilation
  %\listoffigures
  %\listoftables
   
}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Intro
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%% PARSER %%%%%%%%%%%
%\section{Problem description and motivation} \todo{maybe remove this section}
Railway companies need to continuously and sufficiently maintain the train tracks and optimally detect defects in order to have a more punctual and more effective train system. However, the current system is expensive, time consuming and ineffective. That is, maintenance agents need to walk along tracks and check them for defects. For visualisation purposes, there is roughly 5200 km of rails in Switzerland which needs to be inspected by 40 experienced inspectors.


In order to cope with this issue, Swiss Federal Railways (SBB) has specifically built two new special diagnostic vehicles (SDV) designed for defect identification among other purposes. For this, two accelerometers have been installed at the front and back of the vehicle to collect the signal responses from the wheel and the train track
\todo{insert picture, mention boogey?}

A defect in train tracks can be seen as a discontinuity. As a train passes over this discontinuity, it will result in a perturbation that can be detected by sensors. It is our main assumption that each type of defect will have a specific signature that will allow its identification and classification. This is similar to the idea presented in \todo{https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf}

By succesfully identifying and classifying the defects, we take one step further towards reducing delays and making trains more punctual and reliable. The first step in this process consists of identification and classification, while the second step consists of future defect prediction.

\section{Objective}
As the title implies, the objective of this project is to identify and classify rail surface defects. 

apply machine learning techniques on the problem

\section{Defects}
Evidently, a defect can be seen as a deviation from the standard train track. For the exact defect type, SBB has self-constructed a database for the individual defect definitions \todo{is this a recognized system?}. Here is a few examples: \todo{insert pictures}


Generally, a defect is separated into two overarching types: range- and point-defects. I.e. a defect that is detected at a single point versus a defect that is detected at varying lengths -- e.g. \todo{insert example of range vs point}.

For this project, we have solely focused on the point defects for analysis, as this simplifies the problem statement. \todo{show signal types?}. A point defect is perceived as a sharp signal response, whereas a range-defect is perceived over a greater time period. We thus disregard range-defects such that we do not have to deal with the extra, associated factors. 

\todo{See list of defect types in appendix?}


\section{Data}
The data has been collected and provided by SBB. Using their SDV, SBB has made trips back and forth to different cities in Switzerland in order to collect various data including but not limited to accelerometer data. After getting the data from SBB, it then goes through a processing pipeline (designed by Cyprien), after which the data can be manipulated with \verb|python| dataframes (from \verb|pd.DataFrame|). The accelerometer captures the accelerations at the XYZ-axes (along with the timestamps at each recording), of which we are only concerned with the Y-axis for the vertical pertubations.

\todo{make a table of the equipment and sample frequencies}

Furhtermore, the locations of the defects have to be retrieved from SBB's database. which were retrieved by Cyprien.


\todo{Which data was worked on, put in tables, switches, ins joints and defects}
\todo{which track entities are we actually analysing}

We need to define terminlogy of these: defects, ins joints = 
in the following we will use defect as an umbrella term for these entities.

\section{Code}
The code is written purely in python. The code can be found on github: \\
\url{https://github.com/Aiyualive/SemesterProject2.0}.

%mainly uses pandas and stuff


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Design and implementation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Design and Implementation}

For the process of defect classification we employed the following pipeline:
\todo{insert pipeline picture}

In the following, I would like to give an overview of how these was implemented

%\verb|signal --> shift GPS timestamps --> localise defects --> create a library of peak windows --> feed to convolutional NN --> reevaluation| Realised that the results were not very good. So we need to visualise the data and look for separability to make a better model


\todo{show a few entity signals and their features, appendix for more signals?}


\section{Shift of GPS timestamps}
The SDV has its GPS sensor installed at a specified location on the vehicle body. However, what we need to achieve is the position (covered distance) at each accelerometer at either sides of the GPS. Since the GPS sensor is sampled at a lower frequency compared to the accelerometers, we first need to get the corresponding positions for each accelerometer sample. This is done by interpolation using the timestamps of the accelerometers and GPS. 

Depending on the direction of the vehicle we then subtract/add the offset between the accelerometers and the GPS sensor with regard to the position of these sensors on the vehicle body.

\todo{insert drawing of how it is calculated?}

\section{Peak windows}
Retrieving the signal response around the defect location forms a crucial aspect in the overarching pipeline. The goal of this step is to, around each defect, create a "window" containing accelerometer accelerations of a specified time length -- wherein Within the highest acceleration recording around is found in the center. As a result, all of these windows would be uniform in the sense that they are all centered according to the highest recording of a defect. It is then assumed that each window forms the signature of each track entity. 

Since we are assuming that each track entity is identified by a well-formed peak, we first need to find this peak within a reasonable offset from the defect location, after which we center around that within another reasonable offset. 

In the code, this is done by defining two parameters: \verb|find_peak_offset = 1| and \verb|window_offset = 0.5|. I.e. given a defect timestamp, we search for the highest acceleration recording that has occured $1$ second after and $1$ second before the defect timestamp. Once the peak has been found, we then center it in a $1$ second window ($0.5$ sec on each side).

\todo{insert a drawing of how this works?}

\section{Entity library}
The peak windows arguably forms the central feature of the defect library. However, from domain knowledge, other features like speed also needs to be considered for our neural network. Apart from the peak windows, we have also extracted a variety of relevant features that might be useful for classifcation. These include \todo{list, and how was it implemented}.

some can be extracted directly by the original dataframe, some needs some processing.

\section{Neural network architecture}
To create neural network architectures, we are using: \verb|keras| along with \verb|tensorflow|. \verb|keras| is essentially a high-level neural networks library which runs on top of \verb|tensorflow|. It has consistent, a simple API and provides clear and actionable feedback upon user error. Models are easily made by connecting configurable building blocks together, with few restrictions \cite{TensorFl31:online}.

For our use case, we have created a primary \verb|NN| class (short for neural network) along with a \verb|ModelMaker| class. The former does everything from pre-processing the data to evaluating the used model. The latter, as the name suggests, is utlised for creating and using different models, which is useful as we can keep track of how the models have been modified and improved. 

To make a classification, we first need to select the relevant features. Then we simply feed the features into an \verb|NN| object, where the API of the \verb|NN| class can be called for classification. The usage of the \verb|NN| class is demonstrated below in \ref{tab:nnclass}.


\begin{table}[H]
	\centering
	\small
	\begin{tabular}{l p{0.7\linewidth}}\noalign{\global\arrayrulewidth=0.3mm} 
	\hline 
	\multicolumn{2}{c}{\textbf{Workflow of NN class}}
		\\ \hline  
		\noalign{\global\arrayrulewidth=0.05mm}
		\verb|__init__()|                  & initialises a \verb|NN| object\\ \hline
		\verb|prepare_data()|            & pre-process data, this includes standardisation of data\\ \hline
		\verb|make_model()|              & uses \verb|ModelMaker| class to select a model \\ \hline
		
		\verb|fit()|			         & trains the model\\ 
		\noalign{\global\arrayrulewidth=0.3mm}
		\hline
	\end{tabular}
	\caption{To train a model, these functions needs to be called sequentially}\label{tab:nnclass}
\end{table}

\begin{table}[H]
	\label{other}
	\centering
	\small
	\begin{tabular}{l p{0.6\linewidth}}\noalign{\global\arrayrulewidth=0.3mm} \hline
	\multicolumn{2}{c}{\textbf{Other useful functions}}
		\\ \hline 
		\noalign{\global\arrayrulewidth=0.05mm}
		\verb|predict()|                 & used to classify a testing set in the future \\ \hline
		\verb|measure_performance()|     & currently only done on validation data \\ \hline
		\verb|plot_metrics()|            & plots the metrics \\ \hline
		\verb|plot_confusion_matrix()|   & plots the confusion matrix\\ \hline
		\verb|save_history()|            & \\ \hline
		\verb|save_model()|              & \\ 
		\noalign{\global\arrayrulewidth=0.3mm}
		\hline \hline
		\verb|run_experiment()|          & evaluates the given model for a number of times\\ 
		\noalign{\global\arrayrulewidth=0.3mm}
		\hline
	\end{tabular}
	\caption{f}
\end{table}
\todo{insert predict-classify after this?} 
\todo{insert appendix of a description of every function?}


\section{Visualisation}
Finally, after evaluating the results (results can be seen in the next section) from the neural network, we have not achieved any significant results. Arguably, the visualisations of class separability should have been handled first. However, the previous steps took the majority of the time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
Here we will present the results and discuss the findings herein.

\section{Model}
\begin{table}[H]
	\centering
	\captionof{table}{Table Title} \label{tab:title2} 
	\begin{tabular}{|c|c|c| c|} \hline
		\textbf{Layer} & \textbf{Output Shape } & \textbf{Number of params} \\ \hline \hline 
		Synthetic data & 2000 &  9  \\ \hline 

	\end{tabular}
	\par \bigskip Dataset formats
	\label{datasets}
\end{table}

\url{http://alexlenail.me/NN-SVG/AlexNet.html}

\section{Classification results}
\begin{table}[H]
	\centering
\begin{tabular}{|lcccc|}
	\hline
	\backslashbox{run \#}{Model}
&{Model1}&{ClaveVectors}&{MNIST-bin-digits}&{MNIST-all-digits} \\\hline\hline
	1 &  & 61.355 & 3212.497 & 2454.585 \\ \hline
	2 & 16.568 & 62.396 & 3346.047 & 2782.271 \\ \hline
	3 & 14.362 & 59.282 & 2908.446 & 2618.519 \\ \hline
	4 & 13.85 & 62.193 & 3043.189 &  2600.233 \\ \hline
	5 & 13.755 & 61.024 & 3309.422 & 2470.139 \\ \hline
	6 & 12.218 & 58.531 & 3192.178 & 2823.685 \\ \hline
	7 & 12.232 & 60.187 & 3287.289 & 2452.566 \\ \hline
	8 & 13.346 & 59.398 & 3026.486 & 2810.943 \\ \hline
	9 & 11.433 & 61.892 & 3127.583 & 2528.202 \\ \hline
	10 & 11.63 & 60.925 & 3018.402 & 2746.729 \\ \hline
\end{tabular}
	\caption{Architecture of model number}
\end{table}
\todo{insert the different defect types and class distributions, appendix for each result?}
\todo{just do one model at a time and show their metrics
}

\section{Visualisation results}
insert the plots

\section{Discussion}
I tried to increase the outliers, but this was a hugely naive approach

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Conclusion and future work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion and future work}

\section{Conclusion}

\section{Future work}

\begin{itemize}
	\item Might be interesting to also consider the XZ-axes.
	\item range defects
	\item tune the peak finding parameters
	\item track entity dependent/specific window offsets
	\item we must not set the findpeakoffset too high
	\item Questions, what if you want to use multiple features with 1 CNN
	
	
\end{itemize}

\newpage
\section{TODO}
\begin{itemize}
	\item get percentage of each class in the validation set
	\item create an average of the model ie run model for more times
	\item add speed as a feature
	\item what does each filter do? what is kernel size?
	\item very fast speed, overlap between switch and ins, old vs new rail, ax1 arrow 2 arrow 3 arrow 4
	\item 3D plots?
	\item change the defect library to use pandas instead?
	\item visualise what the network is doing using Harry's code
	\item use speed as a feature also
	\item be consistent with function naming and variable names: function names with underscore and variable names with camelcase
	\item save confusion matrix, and metrics (modify the plotting of this) \item citation \cite{benchmark}
	\item Which type of defects are we actually working with, we can see that it does good at the switches and ins joints, but no chance with the defects
	\item we should instead call it an entity library -- make up your mind
	\item plot confusion matrix instead, and classify instead of predict
	\item do objective, and entity thingy
	
\end{itemize}

\bibliography{master}
%\todo{New paper with train}

\newpage
\cleardoublepage
\appendix
% A chapter for each section
\chapter{Implementation}
\begin{table}[H]
	\centering
	\begin{tabular}{l p{0.35\linewidth}} \hline
	\multicolumn{2}{c}{{\large \textbf{Functions in NN class}}}
		\\ \hline  
		prepare data      &  \\ \hline
	\end{tabular}
\end{table}
\chapter{Results}




\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|} \hline
		\textbf{Latent Dimension} & $2$ \\ \hline
		\textbf{Shape} & 8 $\times$ $8$ \\ \hline
		\textbf{Latent Points} &  $64$\\ \hline
		\textbf{Dist. Mixtures} &  Bernoulli\\ \hline \hline
		\textbf{Regularisation $\alpha$ } & $0.001$ \\ \hline
	\end{tabular}
	\hspace*{0.5cm}
	\begin{tabular}{|l|c|} \hline
		\textbf{RBF Dimension} & $2$ \\ \hline
		\textbf{RBF grid}  & $4\times4$ \\ \hline 
		\textbf{RBF centres} & $16$ \\ \hline
		\textbf{Basis function} & Gaussian \\ \hline
	    \textbf{Widths $\bm{\sigma}$} & $1.0$\\ \hline
	\end{tabular}
	\caption{The parameters for the latent model and the RBF neural network. Distribution mixtures and basis functions cannot be altered as our implementation is specific for the Bernoulli GTM version}
	\label{eval:model}
\end{table}


%\label{code}
%\inputminted[linenos, fontsize = \scriptsize]{python}{../src/utils/defect_utils.py}

\end{document}
